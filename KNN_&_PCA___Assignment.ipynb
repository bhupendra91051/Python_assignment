{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-016\n",
        "#KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "dum8YhgCVhDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Answer:\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression tasks. It is a non-parametric and instance-based learning algorithm.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Given a query point, the algorithm finds the K nearest neighbors in the training dataset based on a distance metric (e.g., Euclidean).\n",
        "\n",
        "In classification, the majority label among the K neighbors is assigned.\n",
        "\n",
        "In regression, the output is the average (or weighted average) of the neighbors' values.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Curse of Dimensionality refers to various problems that arise when analyzing and organizing data in high-dimensional spaces.\n",
        "\n",
        "In the context of KNN:\n",
        "\n",
        "As dimensions increase, data points become sparse, making the concept of \"nearness\" less meaningful.\n",
        "\n",
        "Distance metrics lose effectiveness, causing KNN to perform poorly.\n",
        "\n",
        "Computational cost increases with dimensionality.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms original features into new uncorrelated features (principal components) that capture the most variance.\n",
        "\n",
        "Difference from Feature Selection:\n",
        "\n",
        "PCA creates new features (linear combinations), while feature selection selects existing features.\n",
        "\n",
        "PCA is unsupervised, whereas feature selection can be supervised or unsupervised.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Eigenvectors represent the directions (principal components) of the new feature space.\n",
        "\n",
        "Eigenvalues represent the magnitude of variance captured by each eigenvector.\n",
        "\n",
        "They are crucial because:\n",
        "\n",
        "The eigenvector with the highest eigenvalue captures the most variance.\n",
        "\n",
        "Selecting top components based on eigenvalues helps in reducing dimensionality.\n",
        "\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer:\n",
        "\n",
        "PCA helps by reducing noise and dimensionality, addressing the curse of dimensionality.\n",
        "\n",
        "This makes KNN more effective and efficient.\n",
        "\n",
        "Together, PCA + KNN improve model generalization, especially on high-dimensional datasets.  \n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.  \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Dataset:  \n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "lcBmBELoV31v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhQXLuxAVMrD",
        "outputId": "5acad03b-b892-4bcb-8742-acbd0dff4dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.71\n",
            "Accuracy with scaling: 0.96\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# KNN without scaling\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "acc_no_scaling = knn.score(X_test, y_test)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier()\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = knn_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.  \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:"
      ],
      "metadata": {
        "id": "UQ9OHgZOW2Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Scale data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained variance ratio:\")\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He-GAJaGWxpd",
        "outputId": "66655809-2db0-4064-c9fb-9907f01a2e0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.  \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:"
      ],
      "metadata": {
        "id": "K1-3WhW-XEdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA with 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_pca_2 = pca_2.fit_transform(X_scaled)\n",
        "\n",
        "# Train/test split\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca_2, y, random_state=42)\n",
        "\n",
        "# Train KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier()\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "acc_pca = knn_pca.score(X_test_pca, y_test_pca)\n",
        "\n",
        "print(f\"Accuracy with top 2 PCA components: {acc_pca:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZnH1a29W-uj",
        "outputId": "195e9d4c-62b7-4991-dc5c-ffee4e45213c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with top 2 PCA components: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.  \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:  "
      ],
      "metadata": {
        "id": "aj304iiVXQ4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Euclidean (default)\n",
        "knn_euc = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euc.fit(X_train_scaled, y_train)\n",
        "acc_euc = knn_euc.score(X_test_scaled, y_test)\n",
        "\n",
        "# Manhattan\n",
        "knn_man = KNeighborsClassifier(metric='manhattan')\n",
        "knn_man.fit(X_train_scaled, y_train)\n",
        "acc_man = knn_man.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Euclidean accuracy: {acc_euc:.2f}\")\n",
        "print(f\"Manhattan accuracy: {acc_man:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0A_bTY7XMXG",
        "outputId": "a13c1345-e80f-4f77-93df-38a16f53a013"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean accuracy: 0.96\n",
            "Manhattan accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.  \n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.  \n",
        "Explain how you would:  \n",
        "‚óè Use PCA to reduce dimensionality  \n",
        "‚óè Decide how many components to keep  \n",
        "‚óè Use KNN for classification post-dimensionality reduction  \n",
        "‚óè Evaluate the model  \n",
        "‚óè Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data  \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:  \n",
        "When working with high-dimensional gene expression data, where the number of features (genes) is much larger than the number of patient samples, models like KNN can easily overfit due to noise and sparsity. Here's how we can build a robust and interpretable machine learning pipeline using PCA + KNN, especially suited for biomedical use cases.\n",
        "\n",
        " Strategy Overview:  \n",
        "üîπ 1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "Why? Gene expression datasets often contain thousands of features, many of which are correlated or irrelevant.\n",
        "\n",
        "What? PCA helps by transforming the data into a smaller set of uncorrelated variables (principal components) that retain most of the variance.\n",
        "\n",
        "üîπ 2. Decide How Many Components to Keep\n",
        "\n",
        "Use the explained variance ratio to choose the minimum number of components that capture 95% of the variance.\n",
        "\n",
        "This ensures important biological signals are preserved.\n",
        "\n",
        "üîπ 3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "\n",
        "KNN is suitable when features are well-scaled and dimensionality is reduced.\n",
        "\n",
        "PCA reduces overfitting and noise, making KNN effective even with limited samples.\n",
        "\n",
        "üîπ 4. Evaluate the Model\n",
        "\n",
        "Use stratified k-fold cross-validation to get reliable performance metrics.\n",
        "\n",
        "Measure accuracy, and optionally other metrics like precision/recall if classes are imbalanced.\n",
        "\n",
        "üîπ 5. Justify to Stakeholders\n",
        "\n",
        "The pipeline is:\n",
        "\n",
        "Scientifically sound: PCA reduces noise and retains biological signal.\n",
        "\n",
        "Interpretable: Easy to understand how data is transformed and classified.\n",
        "\n",
        "Efficient: Reduces training time and improves generalization.\n",
        "\n",
        "Proven: Widely used in genomic and biomedical research.  \n",
        "Python Code:"
      ],
      "metadata": {
        "id": "PUI-U9F5XeQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Simulate high-dimensional gene expression dataset\n",
        "# 100 samples, 1000 features (like gene expression), 2 cancer types\n",
        "X, y = make_classification(n_samples=100, n_features=1000, n_informative=50,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply PCA to retain 95% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original feature count: {X.shape[1]}\")\n",
        "print(f\"Reduced feature count after PCA: {X_pca.shape[1]}\")\n",
        "print(f\"Total explained variance by selected components: {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
        "\n",
        "# 4. Train and evaluate KNN using 5-fold cross-validation\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "cv_scores = cross_val_score(knn, X_pca, y, cv=5)\n",
        "\n",
        "print(f\"\\nCross-validated accuracy scores: {cv_scores}\")\n",
        "print(f\"Mean accuracy: {cv_scores.mean():.2f}\")\n",
        "print(f\"Standard deviation: {cv_scores.std():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyK27PI-XW4_",
        "outputId": "0f7dacc7-3d2a-4201-85be-85aa80e4fa20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original feature count: 1000\n",
            "Reduced feature count after PCA: 90\n",
            "Total explained variance by selected components: 0.95\n",
            "\n",
            "Cross-validated accuracy scores: [0.7  0.65 0.7  0.6  0.6 ]\n",
            "Mean accuracy: 0.65\n",
            "Standard deviation: 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYlsafb7YLoo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}