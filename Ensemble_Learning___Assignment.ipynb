{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-014\n",
        "#Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "jp8DFtWe9ZAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer:\n",
        "Ensemble learning is a machine learning technique where multiple models are combined to solve a problem and improve overall performance. The idea is that while individual models (called base learners or weak learners) may make errors, combining their predictions can reduce those errors and lead to better results. The key idea is to leverage the diversity of models — their different strengths and weaknesses — so that their collective prediction is more accurate, robust, and less prone to overfitting or underfitting than any single model alone.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:  \n",
        "Bagging and Boosting are both ensemble learning techniques but work differently. Bagging stands for Bootstrap Aggregating. In bagging, multiple models are trained independently on random subsets of the training data (with replacement). Their outputs are then combined, usually by averaging (for regression) or majority voting (for classification). This helps reduce variance and prevents overfitting.\n",
        "\n",
        "Boosting, on the other hand, builds models sequentially. Each new model is trained to correct the errors made by the previous one. In boosting, more weight is given to the data points that were misclassified earlier. This method focuses on reducing bias and improving prediction accuracy by focusing more on difficult examples. However, boosting can be more prone to overfitting if not properly regularized.\n",
        "\n",
        " Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:  \n",
        "Bootstrap sampling is a method of creating random subsets of data by sampling from the original dataset with replacement. This means that some data points may appear more than once in a sample, while others may not appear at all.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling is used to train each decision tree on a different subset of the data. This introduces diversity among the trees because each one sees a slightly different view of the data. The final prediction is made by aggregating the predictions of all the trees. This process reduces variance and increases the model’s ability to generalize to unseen data.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:  \n",
        "Out-of-Bag (OOB) samples are the data points that are not included in a given bootstrap sample. In other words, when a model (such as a tree in a Random Forest) is trained on a bootstrap sample, about 37% of the original data is left out — these are called OOB samples.\n",
        "\n",
        "The OOB score is an internal validation technique used to evaluate the model's performance. Each data point is predicted by the subset of trees that did not train on it. The predictions are then compared with the true labels to calculate an accuracy score. This OOB score provides an unbiased estimate of model performance without needing a separate validation set.\n",
        "\n",
        " Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:  \n",
        "In a single Decision Tree, feature importance is calculated based on how much each feature contributes to reducing impurity (such as Gini impurity or entropy) at each split. However, since a decision tree is sensitive to data and can overfit, the feature importance it provides may not be reliable, especially on small or noisy datasets.\n",
        "\n",
        "In contrast, a Random Forest consists of multiple trees trained on different subsets of data. Feature importance in a Random Forest is averaged across all trees, making it more stable and robust. This reduces the influence of random fluctuations in data and gives a more reliable estimate of which features are truly important for prediction.  \n",
        "\n",
        "\n",
        "Question 6: Write a Python program to:   \n",
        "● Load the Breast Cancer dataset using   \n",
        "sklearn.datasets.load_breast_cancer()   \n",
        "● Train a Random Forest Classifier   \n",
        "● Print the top 5 most important features based on feature importance scores.   \n",
        "(Include your Python code and output in the code box below.)   \n",
        "Answer:  "
      ],
      "metadata": {
        "id": "EXL_Tf8V9sts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z_TsuHv9X-4",
        "outputId": "d63ed9ce-1df7-4372-85ab-378463f0db11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "top_5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:   \n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset   \n",
        "● Evaluate its accuracy and compare with a single Decision Tree   \n",
        "(Include your Python code and output in the code box below.)   \n",
        "Answer:  "
      ],
      "metadata": {
        "id": "DpPyI2T3_udX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_preds = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Tree as base estimator\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_preds = bag_model.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_preds)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Single Decision Tree Accuracy:\", round(dt_accuracy, 2))\n",
        "print(\"Bagging Classifier Accuracy:\", round(bag_accuracy, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZXMFgZh_nvi",
        "outputId": "da925f8d-81ec-48c8-e0bb-75438f53f1e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:   \n",
        "● Train a Random Forest Classifier   \n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV   \n",
        "● Print the best parameters and final accuracy   \n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:"
      ],
      "metadata": {
        "id": "oj1XxxwuAPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", round(accuracy, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCFtTuVd_3Do",
        "outputId": "e001960b-160a-4794-af4b-e4cf1f63b8d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 150}\n",
            "Test Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:   \n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset   \n",
        "● Compare their Mean Squared Errors (MSE)   \n",
        "(Include your Python code and output in the code box below.)   \n",
        "Answer:"
      ],
      "metadata": {
        "id": "ZzYmWT93Aj09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukTar18OAa4x",
        "outputId": "6281bf5d-c5bd-4ab3-c32f-81dfa3ae7cef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2824\n",
            "Random Forest Regressor MSE: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan   default. You have access to customer demographic and transaction history data.   \n",
        "You decide to use ensemble techniques to increase model performance.   \n",
        "Explain your step-by-step approach to:  \n",
        "● Choose between Bagging or Boosting  \n",
        "● Handle overfitting  \n",
        "● Select base models  \n",
        "● Evaluate performance using cross-validation  \n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.  \n",
        "\n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:\n",
        "\n",
        "Scenario:  \n",
        "As a data scientist at a financial institution, your goal is to predict loan default using customer demographic and transaction data. To improve model performance, you decide to apply ensemble learning techniques.\n",
        "\n",
        "Below is the step-by-step approach covering all the points requested:\n",
        "\n",
        " 1. Choose Between Bagging and Boosting\n",
        "\n",
        "Approach:\n",
        "\n",
        "Start with Bagging (e.g., Random Forest) as a baseline model. It's stable, handles noise well, and performs well even with little tuning.\n",
        "\n",
        "Move to Boosting (e.g., XGBoost or LightGBM) for improved accuracy, especially in imbalanced datasets (e.g., default vs non-default).\n",
        "\n",
        "Since loan default prediction is a high-stakes classification problem, Boosting is usually preferred due to its focus on hard-to-classify instances.\n",
        "\n",
        " 2. Handle Overfitting\n",
        "\n",
        "Techniques Used:\n",
        "\n",
        "Use regularization parameters like max_depth, min_child_weight, and learning_rate (in boosting).\n",
        "\n",
        "Use early stopping to stop training when the validation loss stops improving.\n",
        "\n",
        "Apply cross-validation to monitor model generalization.\n",
        "\n",
        "Ensure feature selection and remove highly correlated or irrelevant features.\n",
        "\n",
        " 3. Select Base Models\n",
        "\n",
        "Approach:\n",
        "\n",
        "For Bagging: Use Decision Trees as base learners.\n",
        "\n",
        "For Boosting: Also use shallow Decision Trees (max_depth = 3–5) as weak learners.\n",
        "\n",
        "You may also experiment with logistic regression as a baseline non-ensemble model for comparison.\n",
        "\n",
        " 4. Evaluate Performance Using Cross-Validation\n",
        "\n",
        "Approach:\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation to ensure class distribution is preserved in all folds.\n",
        "\n",
        "Use metrics like accuracy, precision, recall, F1-score, and ROC-AUC, especially because class imbalance is common in loan default datasets.\n",
        "\n",
        " 5. Justify Ensemble Learning for Loan Default Prediction\n",
        "\n",
        "Justification:\n",
        "\n",
        "Ensemble models (especially Boosting) reduce both bias and variance.\n",
        "\n",
        "They are robust to noise, missing values, and outliers — which are common in financial data.\n",
        "\n",
        "Boosting methods adaptively focus on misclassified cases (like defaulters), improving recall — which is critical for identifying risky customers.\n",
        "\n",
        "This improves decision-making, reduces loan default risk, and enhances financial stability.\n",
        "\n",
        "Python Code Example (using XGBoost as Boosting model)"
      ],
      "metadata": {
        "id": "pFFs1zkzAz9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Simulated dataset (replace with your real loan dataset)\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=10, n_redundant=5,\n",
        "    random_state=42, weights=[0.7, 0.3]\n",
        ")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Train XGBoost classifier (boosting)\n",
        "model = XGBClassifier(\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", round(acc, 2))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Cross-validation score\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(\"\\nCross-Validation Accuracy:\", round(cv_scores.mean(), 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIsRBy-2AqZW",
        "outputId": "6208336e-34b1-4237-f6c8-12c2b9f28302"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.94\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       139\n",
            "           1       0.95      0.85      0.90        61\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.94      0.92      0.93       200\n",
            "weighted avg       0.94      0.94      0.94       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [22:57:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXCC0oDQC26I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}