{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-013\n",
        "#SVM & Naive Bayes | Assignment"
      ],
      "metadata": {
        "id": "v6m2pAd8CmMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Answer:  \n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates the data points of different classes with the maximum margin. The data points closest to the hyperplane are called support vectors, and they are critical for defining the boundary. SVM can efficiently handle high-dimensional data and is effective in cases where the number of features exceeds the number of samples.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Answer:  \n",
        "\n",
        "Hard Margin SVM assumes that the data is linearly separable and tries to find a hyperplane that perfectly separates the classes without any misclassification. It does not allow any margin violations.\n",
        "\n",
        "Soft Margin SVM allows for some misclassification to achieve better generalization. It introduces a penalty parameter C to control the trade-off between maximizing the margin and minimizing classification errors. Soft Margin is more practical and robust, especially when data is noisy or overlapping.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "Answer:  \n",
        "The Kernel Trick is a technique in SVM that allows it to operate in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. Instead, it uses a kernel function to calculate the inner product between two data points in the transformed space.\n",
        "\n",
        "Example – RBF (Radial Basis Function) Kernel:\n",
        "The RBF kernel is useful for handling non-linear relationships. It measures the similarity between points and maps them into an infinite-dimensional space, allowing the SVM to find complex decision boundaries. It's commonly used when data is not linearly separable.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Answer:\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning model based on Bayes’ Theorem. It is primarily used for classification tasks, especially in text classification problems such as spam detection, sentiment analysis, etc.\n",
        "\n",
        "The classifier assumes that the presence (or absence) of one feature is independent of the presence (or absence) of any other feature, given the class label. This is a strong assumption and often not true in real-world data, which is why the algorithm is called \"naïve\".\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?   \n",
        "Answer:    \n",
        "Naïve Bayes classifiers are a family of probabilistic models based on Bayes’ Theorem with the assumption of conditional independence between features. There are several variants of Naïve Bayes, each tailored to different types of data. The most common ones are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "Description:\n",
        "Assumes that the features follow a normal (Gaussian) distribution. For each feature, the likelihood of the data is estimated using the mean and standard deviation of the feature in the training data.\n",
        "\n",
        "Use case:\n",
        "Suitable for continuous numeric data where features are assumed to be normally distributed.\n",
        "\n",
        "Example:\n",
        "Predicting whether a patient has a disease based on lab measurements like blood pressure, cholesterol levels, etc.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Description:\n",
        "Designed for discrete count data. It models the data using multinomial distributions and is typically used when features represent the frequencies or counts of events.\n",
        "\n",
        "Use case:\n",
        "Commonly used in text classification tasks (like spam detection or sentiment analysis) where features represent word counts or term frequencies (e.g., in a bag-of-words model).\n",
        "\n",
        "Example:\n",
        "Classifying emails as spam or not based on word occurrence counts.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Description:\n",
        "Assumes binary features, where each feature is either present (1) or absent (0). It models the data using Bernoulli distributions.\n",
        "\n",
        "Use case:\n",
        "Also used in text classification, but when features are binary—i.e., whether a word appears in a document or not, regardless of frequency.\n",
        "\n",
        "Example:\n",
        "Detecting whether a document belongs to a topic based on the presence or absence of certain keywords.\n",
        "\n",
        "When to Use Each:\n",
        "\n",
        "Use Gaussian NB when your features are continuous and roughly normally distributed.\n",
        "\n",
        "Use Multinomial NB when your features are discrete counts (e.g., word frequencies).\n",
        "\n",
        "Use Bernoulli NB when your features are binary indicators (e.g., presence/absence of a feature).\n",
        "\n",
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)  \n",
        "\n",
        "Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.  \n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZXAcfGHEDL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0E7f0TiCihV",
        "outputId": "f43d6fb7-354c-4024-de23-8150ea0788f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"SVM Model Accuracy:\", accuracy)\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)  \n",
        "Answer:"
      ],
      "metadata": {
        "id": "FgS8hKvHGaP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhm5XB5PGQPf",
        "outputId": "56eefd38-d698-47ca-82ec-d8688f1078d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "T6yWRzmGGn8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
        "    'gamma': ['scale', 0.001, 0.01, 0.1, 1],  # Kernel coefficient\n",
        "    'kernel': ['rbf']               # Use RBF kernel for gamma parameter relevance\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test data using the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhDYvjHTGhRj",
        "outputId": "c3077f5e-1d78-4afd-efb2-bdc76569b14e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)   \n",
        "Answer:"
      ],
      "metadata": {
        "id": "6KclO2B6G21Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (choose only two categories to make it binary classification)\n",
        "categories = ['comp.graphics', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target  # binary labels: 0 or 1\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Multinomial Naïve Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_probs = nb.predict_proba(X_test_tfidf)[:, 1]  # Probability for positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2zN1Ag7HDds",
        "outputId": "93ad3437-5a17-4081-9175-ad69a5f02ad8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.  \n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:   \n",
        "● Text with diverse vocabulary   \n",
        "● Potential class imbalance (far more legitimate emails than spam)   \n",
        "● Some incomplete or missing data   \n",
        "Explain the approach you would take to:   \n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)   \n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)   \n",
        "● Address class imbalance   \n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.   \n",
        "(Include your Python code and output in the code box below.)   \n",
        "Answer:  \n",
        "Approach to Spam Email Classification:\n",
        "1. Preprocessing the Data\n",
        "\n",
        "Text Vectorization:\n",
        "Use TF-IDF Vectorizer to convert emails into numerical features that capture term importance and reduce the impact of common words. This is effective for diverse vocabulary in emails.\n",
        "\n",
        "Handling Missing Data:\n",
        "\n",
        "Drop or fill missing email bodies (e.g., with empty strings) since text is crucial.\n",
        "\n",
        "For missing metadata, either impute or exclude those features.\n",
        "\n",
        "Since text data is sparse, TF-IDF can handle it well after filling missing entries.\n",
        "\n",
        "Text Cleaning:\n",
        "Lowercasing, removing punctuation, stop words removal, and possibly stemming or lemmatization to reduce noise.\n",
        "\n",
        "2. Model Choice: SVM vs. Naïve Bayes\n",
        "\n",
        "Naïve Bayes:\n",
        "\n",
        "Fast, simple, and performs well with high-dimensional sparse text data.\n",
        "\n",
        "Good baseline, especially with bag-of-words or TF-IDF features.\n",
        "\n",
        "Assumes feature independence, which is not always true but works well in practice for spam detection.\n",
        "\n",
        "SVM:\n",
        "\n",
        "Can find complex decision boundaries and often outperforms Naïve Bayes on text classification when tuned properly.\n",
        "\n",
        "Can handle large feature spaces but may require more computation and careful tuning (e.g., kernel, regularization).\n",
        "\n",
        "Allows use of class weights to address imbalance.\n",
        "\n",
        "Recommendation: Start with Multinomial Naïve Bayes for speed and simplicity; move to SVM if more accuracy is needed.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "\n",
        "Use resampling techniques such as SMOTE or Random Oversampling to balance classes during training.\n",
        "\n",
        "Alternatively or additionally, use class weights in models (e.g., class_weight='balanced' in SVM).\n",
        "\n",
        "Tune decision thresholds to balance precision and recall depending on business priorities (e.g., minimize false negatives).\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Recall: Important to catch as many spam emails as possible (minimize false negatives).\n",
        "\n",
        "Precision: Important to avoid marking legitimate emails as spam (minimize false positives).\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC: Overall model discrimination ability.\n",
        "\n",
        "Confusion Matrix: For detailed error analysis.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Better spam filtering improves user experience, reduces risk from phishing/malicious emails, and increases trust in the email platform.\n",
        "\n",
        "Automating spam classification saves manual labor and reduces operational costs.\n",
        "\n",
        "Helps maintain brand reputation and customer satisfaction.  \n",
        "\n",
        "Example Python Code:"
      ],
      "metadata": {
        "id": "eo00OwMaHJbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Select two categories to simulate Spam vs Not Spam\n",
        "categories = ['rec.sport.hockey', 'talk.politics.misc']\n",
        "\n",
        "# Load the data\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target  # 0 or 1\n",
        "\n",
        "# Split dataset with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Build a pipeline: TF-IDF vectorizer + Linear SVM with class_weight balanced\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
        "    ('svm', LinearSVC(class_weight='balanced', random_state=42))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPx9BotXGvnN",
        "outputId": "cdceaf43-592d-42d7-e7ea-d6d2592d5f27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94       200\n",
            "           1       0.89      0.98      0.93       155\n",
            "\n",
            "    accuracy                           0.94       355\n",
            "   macro avg       0.94      0.94      0.94       355\n",
            "weighted avg       0.94      0.94      0.94       355\n",
            "\n",
            "Confusion Matrix:\n",
            " [[181  19]\n",
            " [  3 152]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1IekyeTIGEi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}